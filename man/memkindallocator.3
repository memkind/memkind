.\" Automatically generated by Pandoc 2.5
.\"
.TH "MEMKINDALLOCATOR" "3" "2022-08-22" "MEMKINDALLOCATOR | MEMKIND Programmer's Manual"
.hy
.\" SPDX-License-Identifier: BSD-2-Clause
.\" Copyright "2019-2022", Intel Corporation
.SH NAME
.PP
\f[B]libmemkind::static_kind::allocator\f[R] \- The C++ allocator
compatible with the C++ standard library allocator concepts.
.PP
\f[B]Note:\f[R] \f[I]memkind_allocator.h\f[R] functionality is
considered as a stable API (STANDARD API).
.SH SYNOPSIS
.IP
.nf
\f[C]
#include <memkind_allocator.h>

Link with \-lmemkind

libmemkind::static_kind::allocator(libmemkind::kinds kind);
template <typename U> libmemkind::static_kind::allocator<T>::allocator(const libmemkind::static_kind::allocator<U>&) noexcept;
template <typename U> libmemkind::static_kind::allocator(const allocator<U>&& other) noexcept;
libmemkind::static_kind::allocator<T>::\[ti]allocator();
T *libmemkind::static_kind::allocator<T>::allocate(std::size_t n) const;
void libmemkind::static_kind::allocator<T>::deallocate(T *p, std::size_t n) const;
template <class U, class... Args> void libmemkind::static_kind::allocator<T>::construct(U *p, Args... args) const;
void libmemkind::static_kind::allocator<T>::destroy(T *p) const;
\f[R]
.fi
.SH DESCRIPTION
.TP
.B \f[C]libmemkind::static_kind::allocator<T>\f[R]
is intended to be used with STL containers to allocate from static kinds
of memory.
All public member types and functions correspond to standard library
allocator concepts and definitions.
The current implementation supports the C++11 standard.
.IP \[bu] 2
Template arguments:
.RS 2
.IP \[bu] 2
T is an object type aliased by value_type,
.IP \[bu] 2
U is an object type.
Memory management is based on the memkind library.
Refer to the \f[B]memkind\f[R] (/memkind/manpages/memkind.3/)(3) man
page for more details.
.RE
.TP
.B \f[C]T *libmemkind::static_kind::allocator<T>::allocate(std::size_t n)\f[R]
allocates uninitialized memory of size \f[I]n\f[R] bytes of the
specified kind using \f[C]memkind_malloc()\f[R].
Throw \f[B]std::bad_alloc\f[R] when n = 0 or there is not enough memory
to satisfy the request.
.TP
.B \f[C]libmemkind::static_kind::allocator<T>::deallocate(T *p, std::size_t n)\f[R]
deallocates memory associated with pointer returned by
\f[C]allocate()\f[R] using \f[C]memkind_free()\f[R].
.TP
.B \f[C]libmemkind::kinds\f[R]
specifies allocator static kinds of memory, representing type of memory
which offers different characteristics.
The available types of allocator kinds of memory:
.SS Types of allocator kinds of memory
.TP
.B \f[C]libmemkind::kinds::DEFAULT\f[R]
The default allocation using standard memory and the default page size.
The allocation can be made using any NUMA node containing memory.
.TP
.B \f[C]libmemkind::kinds::HIGHEST_CAPACITY\f[R]
Allocate from a NUMA node(s) that has the highest capacity among all
nodes in the system.
.TP
.B \f[C]libmemkind::kinds::HIGHEST_CAPACITY_PREFERRED\f[R]
Same as \f[C]libmemkind::kinds::HIGHEST_CAPACITY\f[R] except that if
there is not enough memory in the NUMA node that has the highest
capacity in the local domain to satisfy the request, the allocation will
fall back on other memory NUMA nodes.
\f[B]Note:\f[R] For this kind, the allocation will not succeed if there
are two or more NUMA nodes that have the highest capacity.
.TP
.B \[ga]libmemkind::kinds::HIGHEST_CAPACITY_LOCAL
Allocate from a NUMA node that has the highest capacity among all NUMA
Nodes from the local domain.
NUMA Nodes have the same local domain for a set of CPUs associated with
them, e.g.\ socket or sub\-NUMA cluster.
\f[B]Note:\f[R] If there are multiple NUMA nodes in the same local
domain that have the highest capacity \- the allocation will be done
from a NUMA node with worse latency attribute.
This kind requires locality information described in the SYSTEM
CONFIGURATION section.
.TP
.B \f[C]libmemkind::kinds::HIGHEST_CAPACITY_LOCAL_PREFERRED\f[R]
Same as \f[C]libmemkind::kinds::HIGHEST_CAPACITY_LOCAL\f[R] except that
if there is not enough memory in the NUMA node that has the highest
capacity to satisfy the request, the allocation will fall back on other
memory NUMA nodes.
.TP
.B \f[C]libmemkind::kinds::LOWEST_LATENCY_LOCAL\f[R]
Allocate from a NUMA node that has the lowest latency among all NUMA
Nodes from the local domain.
NUMA Nodes have the same local domain for a set of CPUs associated with
them, e.g.\ socket or sub\-NUMA cluster.
\f[B]Note:\f[R] If there are multiple NUMA nodes in the same local
domain that have the lowest latency \- the allocation will be done from
a NUMA node with smaller memory capacity.
This kind requires locality and memory performance characteristics
information described in the SYSTEM CONFIGURATION section.
.TP
.B \f[C]libmemkind::kinds::LOWEST_LATENCY_LOCAL_PREFERRED\f[R]
Same as \f[C]libmemkind::kinds::LOWEST_LATENCY_LOCAL\f[R] except that if
there is not enough memory in the NUMA node that has the lowest latency
to satisfy the request, the allocation will fall back on other memory
NUMA nodes.
.TP
.B \f[C]libmemkind::kinds::HIGHEST_BANDWIDTH_LOCAL\f[R]
Allocate from a NUMA node that has the highest bandwidth among all NUMA
Nodes from the local domain.
NUMA Nodes have the same local domain for a set of CPUs associated with
them, e.g.\ socket or sub\-NUMA cluster.
\f[B]Note:\f[R] If there are multiple NUMA nodes in the same local
domain that have the highest bandwidth \- the allocation will be done
from a NUMA node with smaller memory capacity.
This kind requires locality and memory performance characteristics
information described in the SYSTEM CONFIGURATION section.
.TP
.B \f[C]libmemkind::kinds::HIGHEST_BANDWIDTH_LOCAL_PREFERRED\f[R]
Same as \f[C]libmemkind::kinds::HIGHEST_BANDWIDTH_LOCAL\f[R] except that
if there is not enough memory in the NUMA node that has the highest
bandwidth to satisfy the request, the allocation will fall back on other
memory NUMA nodes.
.TP
.B \f[C]libmemkind::kinds::HUGETLB\f[R]
Allocate from standard memory using huge pages.
\f[B]Note:\f[R] This kind requires huge pages configuration described in
the SYSTEM CONFIGURATION section.
.TP
.B \f[C]libmemkind::kinds::INTERLEAVE\f[R]
Allocate pages interleaved across all NUMA nodes with transparent huge
pages disabled.
.TP
.B \f[C]libmemkind::kinds::HBW\f[R]
Allocate from the closest high bandwidth memory NUMA node at the time of
allocation.
If there is not enough high bandwidth memory to satisfy the request,
\f[I]errno\f[R] is set to \f[B]ENOMEM\f[R] and the allocated pointer is
set to NULL.
\f[B]Note:\f[R] This kind requires memory performance characteristics
information described in the SYSTEM CONFIGURATION section.
.TP
.B \f[C]libmemkind::kinds::HBW_ALL\f[R]
Same as \f[C]libmemkind::kinds::HBW\f[R] except decision regarding
closest NUMA node is postponed until the time of the first write.
.TP
.B \f[C]libmemkind::kinds::HBW_HUGETLB\f[R]
Same as \f[C]libmemkind::kinds::HBW\f[R] except the allocation is backed
by huge pages.
Note: This kind requires huge pages configuration described in the
SYSTEM CONFIGURATION section.
.TP
.B \f[C]libmemkind::kinds::HBW_ALL_HUGETLB\f[R]
Combination of \f[C]libmemkind::kinds::HBW_ALL\f[R] and
\f[C]libmemkind::kinds::HBW_HUGETLB\f[R] properties.
\f[B]Note:\f[R] This kind requires huge pages configuration described in
the SYSTEM CONFIGURATION section.
.TP
.B \f[C]libmemkind::kinds::HBW_PREFERRED\f[R]
Same as \f[C]libmemkind::kinds::HBW\f[R] except that if there is not
enough high bandwidth memory to satisfy the request, the allocation will
fall back on standard memory.
.TP
.B \f[C]libmemkind::kinds::HBW_PREFERRED_HUGETLB\f[R]
Same as \f[C]libmemkind::kinds::HBW_PREFERRED\f[R] except the allocation
is backed by huge pages.
\f[B]Note:\f[R] This kind requires huge pages configuration described in
the SYSTEM CONFIGURATION section.
.TP
.B \f[C]libmemkind::kinds::HBW_INTERLEAVE\f[R]
Same as \f[C]libmemkind::kinds::HBW\f[R] except that the pages that
support the allocation are interleaved across all high bandwidth nodes
and transparent huge pages are disabled.
.TP
.B \f[C]libmemkind::kinds::REGULAR\f[R]
Allocate from regular memory using the default page size.
Regular means general purpose memory from the NUMA nodes containing
CPUs.
.TP
.B \f[C]libmemkind::kinds::DAX_KMEM\f[R]
Allocate from the closest persistent memory NUMA node at the time of
allocation.
If there is not enough memory in the closest persistent memory NUMA node
to satisfy the request, \f[I]errno\f[R] is set to \f[B]ENOMEM\f[R] and
the allocated pointer is set to NULL.
.TP
.B \f[C]libmemkind::kinds::DAX_KMEM_ALL\f[R]
Allocate from the closest persistent memory NUMA node available at the
time of allocation.
If there is not enough memory on any of persistent memory NUMA nodes to
satisfy the request, \f[I]errno\f[R] is set to \f[B]ENOMEM\f[R] and the
allocated pointer is set to NULL.
.TP
.B \f[C]libmemkind::kinds::DAX_KMEM_PREFERRED\f[R]
Same as \f[C]libmemkind::kinds::DAX_KMEM\f[R] except that if there is
not enough memory in the closest persistent memory NUMA node to satisfy
the request, the allocation will fall back on other memory NUMA nodes.
\f[B]Note:\f[R] For this kind, the allocation will not succeed if two or
more persistent memory NUMA nodes are in the same shortest distance to
the same CPU on which process is eligible to run.
Check on that eligibility is done upon starting the application.
.TP
.B \f[C]libmemkind::kinds::DAX_KMEM_INTERLEAVE\f[R]
Same as \f[C]libmemkind::kinds::DAX_KMEM\f[R] except that the pages that
support the allocation are interleaved across all persistent memory NUMA
nodes.
.SH SYSTEM CONFIGURATION
.TP
.B HUGETLB (huge pages)
Interfaces for obtaining 2MB (\f[B]HUGETLB\f[R]) memory need allocated
huge pages in the kernel\[cq]s huge page pool.
Current number of \[lq]persistent\[rq] huge pages can be read from the
\f[I]/proc/sys/vm/nr_hugepages\f[R] file.
Proposed way of setting hugepages is:
\f[C]sudo sysctl vm.nr_hugepages=<number_of_hugepages>\f[R].
More information can be found here:
<https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt>
.TP
.B Locality information
Interfaces for obtaining locality information are provided by
\f[I]libhwloc\f[R] dependency.
Functionality based on locality requires that memkind library is
configured and built with the support of the
\f[I]libhwloc\f[R] (https://www.open-mpi.org/projects/hwloc) :
.PD 0
.P
.PD
\f[C]./configure \-\-enable\-hwloc\f[R]
.TP
.B Memory performance characteristics information
Interfaces for obtaining memory performance characteristics information
are based on \f[I]HMAT\f[R] (Heterogeneous Memory Attribute Table).
See
<https://uefi.org/sites/default/files/resources/ACPI_6_3_final_Jan30.pdf>
for more information.
Functionality based on memory performance characteristics requires that
platform configuration fully supports \f[I]HMAT\f[R] and memkind library
is configured and built with the support of the
\f[I]libhwloc\f[R] (https://www.open-mpi.org/projects/hwloc) :
.PD 0
.P
.PD
\f[C]./configure \-\-enable\-hwloc\f[R]
.PP
\f[B]Note:\f[R] For a given target NUMA Node, the OS exposes only the
performance characteristics of the best performing NUMA node.
.PP
\f[I]libhwloc\f[R] can be reached on:
<https://www.open-mpi.org/projects/hwloc>
.SH COPYRIGHT
.PP
Copyright (C) 2019 \- 2022 Intel Corporation.
All rights reserved.
.SH SEE ALSO
.PP
\f[B]memkind\f[R] (/memkind/manpages/memkind.3/)(3)
